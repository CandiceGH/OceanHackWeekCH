{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing multiple visualization libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import mlab\n",
    "import pylab as pl\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries to manipulate the data files\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a library to read the .aiff format\n",
    "import aifc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob(os.path.join('whale_data','train','*.aiff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data\n",
    "fs = 2000 # frequency\n",
    "XX = np.zeros((X.shape[0],129)).astype(\"float32\")   # allocate space\n",
    "for i in range(X.shape[0]):\n",
    "    XX[i] = 10*np.log10(signal.welch(X[i], fs=fs, window='hanning', nperseg=256, noverlap=128+64)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read signals and store as numpy arrays\n",
    "feature_dict = {}\n",
    "fs = 2000\n",
    "for filename in filenames[::1]:\n",
    "    aiff = aifc.open(filename,'r')\n",
    "    whale_strSig = aiff.readframes(aiff.getnframes())\n",
    "    whale_array = np.fromstring(whale_strSig, np.short).byteswap()\n",
    "    feature = 10*np.log10(signal.welch(whale_array, fs=fs, window='hanning', nperseg=256, noverlap=128+64)[1])\n",
    "    feature_dict[filename] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 30000)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "XX = pd.DataFrame(feature_dict)\n",
    "XX = np.array(XX)\n",
    "XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Deep learning on time domain samples.\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the stored data\n",
    "X = np.load('X.npy').T\n",
    "y = np.load('Y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = y.astype('str')\n",
    "y = y.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 129)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target_names = ['Upcall', 'NO_Upcall']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX.T, y, test_size=0.20, random_state=2018)\n",
    "\n",
    "# Convert label to onehot\n",
    "#y_train = keras.utils.to_categorical(y_train, num_classes=2)\n",
    "#y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Neural Network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(16, 3, activation='relu', input_shape=(129, 1)))\n",
    "model.add(Conv1D(16, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv1D(32, 3, activation='relu'))\n",
    "model.add(Conv1D(32, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(256, 3, activation='relu'))\n",
    "model.add(Conv1D(256, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'deep_1'\n",
    "top_weights_path = 'model_' + str(model_name) + '.h5'\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(top_weights_path, monitor = 'val_acc', verbose = 1, save_best_only = True, save_weights_only = True), \n",
    "    EarlyStopping(monitor = 'val_acc', patience = 6, verbose = 1),\n",
    "    ReduceLROnPlateau(monitor = 'val_acc', factor = 0.1, patience = 3, verbose = 1),\n",
    "    CSVLogger('model_' + str(model_name) + '.log')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Max Pooling/ Average Pooling\n",
    "* Batch normalization\n",
    "* Epochs\n",
    "* Adam Optimizer\n",
    "* Convolutional Layers\n",
    "* Cross Entropy\n",
    "* Adam Optimizer\n",
    "* ReLU\n",
    "* Batch Size\n",
    "* Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking tensorflow version\n",
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "24000/24000 [==============================] - 90s 4ms/step - loss: 0.5571 - acc: 0.7188 - val_loss: 0.4148 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.80400, saving model to model_deep_1.h5\n",
      "Epoch 2/100\n",
      "24000/24000 [==============================] - 83s 3ms/step - loss: 0.4246 - acc: 0.8145 - val_loss: 0.4416 - val_acc: 0.7882\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.80400\n",
      "Epoch 3/100\n",
      "24000/24000 [==============================] - 83s 3ms/step - loss: 0.3800 - acc: 0.8341 - val_loss: 0.3598 - val_acc: 0.8400\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.80400 to 0.84000, saving model to model_deep_1.h5\n",
      "Epoch 4/100\n",
      "24000/24000 [==============================] - 83s 3ms/step - loss: 0.3602 - acc: 0.8417 - val_loss: 0.3607 - val_acc: 0.8433\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.84000 to 0.84333, saving model to model_deep_1.h5\n",
      "Epoch 5/100\n",
      "24000/24000 [==============================] - 83s 3ms/step - loss: 0.3454 - acc: 0.8461 - val_loss: 0.4137 - val_acc: 0.8158\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.84333\n",
      "Epoch 6/100\n",
      "24000/24000 [==============================] - 83s 3ms/step - loss: 0.3392 - acc: 0.8481 - val_loss: 0.3585 - val_acc: 0.8368\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.84333\n",
      "Epoch 7/100\n",
      "24000/24000 [==============================] - 83s 3ms/step - loss: 0.3285 - acc: 0.8555 - val_loss: 0.4814 - val_acc: 0.7848\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.84333\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 8/100\n",
      "24000/24000 [==============================] - 83s 3ms/step - loss: 0.3195 - acc: 0.8582 - val_loss: 0.3222 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.84333 to 0.85767, saving model to model_deep_1.h5\n",
      "Epoch 9/100\n",
      "24000/24000 [==============================] - 85s 4ms/step - loss: 0.3135 - acc: 0.8630 - val_loss: 0.3208 - val_acc: 0.8570\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.85767\n",
      "Epoch 10/100\n",
      "24000/24000 [==============================] - 984s 41ms/step - loss: 0.3148 - acc: 0.8616 - val_loss: 0.3189 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.85767 to 0.85833, saving model to model_deep_1.h5\n",
      "Epoch 11/100\n",
      "19200/24000 [=======================>......] - ETA: 15s - loss: 0.3107 - acc: 0.8635"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fitting the Model (this will take a loooooooot of time)\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=100, validation_data = [X_test, y_test], callbacks = callbacks_list)\n",
    "\n",
    "\n",
    "model.load_weights(top_weights_path)\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=16)\n",
    "\n",
    "#print('loss', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "model.fit(x_train, y_train, verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[Deep Learning Glossary](http://www.wildml.com/deep-learning-glossary/)\n",
    "\n",
    "[Keras and NN Tutorial](https://indico.cern.ch/event/506145/contributions/2132944/attachments/1258124/1858154/NNinKeras_MPaganini.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free GPU usage: Google Colaboratory notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
